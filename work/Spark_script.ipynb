{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019fc86-9939-4339-8b48-c6bf8b542d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cc76c7-2744-4717-82b3-f46eac767468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ANTONYMS: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- PARTS_OF_SPEECH: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- SYNONYMS: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- WORD: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import json\n",
    "\n",
    "def transform_json_files(directory_path):\n",
    "    \n",
    "    result = []\n",
    "    # traverse directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # if path leads to file\n",
    "        if os.path.isfile(file_path):\n",
    "            # open file and read as json\n",
    "            with open(file_path, \"r\") as file:\n",
    "                file_json = json.load(file)\n",
    "            # loop through json dictionary entries\n",
    "            for key, value in file_json.items():\n",
    "                value['WORD'] = key\n",
    "                value['PARTS_OF_SPEECH'] = [meaning[0] for meaning in value['MEANINGS'].values() if len(meaning) > 0]\n",
    "                del value['MEANINGS']\n",
    "                result.append(value)\n",
    "    with open(f\"{directory_path}/spark/all_words.json\", \"w\") as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "transform_json_files(\"dictionary_script/data/Dictionary JSON\")\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.json(\"dictionary_script/data/Dictionary JSON/spark/all_words.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0381f1-b582-4243-a890-3b1f1de88773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+------------------+\n",
      "|word|parts_of_speech|adjective_meanings|\n",
      "+----+---------------+------------------+\n",
      "|   A|   [Noun, Noun]|                 0|\n",
      "|A.D.|             []|                 0|\n",
      "|A.M.|             []|                 0|\n",
      "|  AA|         [Noun]|                 0|\n",
      "| AAA|             []|                 0|\n",
      "+----+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "There are 12801 words with at least 1 adjectival meaning out of 121340 words.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "df.createOrReplaceTempView(\"tableA\")\n",
    "\n",
    "def get_number_of_adjectives(list_of_parts):\n",
    "    return len([part for part in list_of_parts if part == \"Adjective\"])\n",
    "\n",
    "spark.udf.register(\"numberOfAdjectives\", get_number_of_adjectives, IntegerType())\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "select word, \n",
    "    parts_of_speech, \n",
    "    numberOfAdjectives(parts_of_speech) as adjective_meanings \n",
    "        from tableA\n",
    "\"\"\")\n",
    "\n",
    "result.show(5)\n",
    "\n",
    "# How many adjectives are there or rather how many words \n",
    "# have at least one context (meaning) where they are adjectives\n",
    "adjectives_count = result.filter(result.adjective_meanings > 0).count()\n",
    "all_words_count = result.count()\n",
    "print(f\"There are {adjectives_count} words with at least 1 adjectival meaning out of {all_words_count} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a45fbc8d-9287-4b4b-810e-d4b4836cf31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for new events...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Kafka_Helpers.Consumer at 0x7f9eec04f7c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_send_back(key, value):\n",
    "    import ast\n",
    "    from pyspark import SparkContext\n",
    "    from Kafka_Helpers import Producer\n",
    "    import json\n",
    "    \n",
    "    value = value.replace(\"\\'\", \"\\\"\")\n",
    "    o = json.loads(value)\n",
    "    \n",
    "    # Create Spark context\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    # make list of lists out of string\n",
    "    l = ast.literal_eval(str(o['reviews']))\n",
    "    \n",
    "    # flatten list of lists to one single list\n",
    "    l = [item for sublist in l for item in sublist]\n",
    "    \n",
    "    # create Spark RDD from flattened list\n",
    "    wordsRDD = sc.parallelize(l, 4)\n",
    "    \n",
    "    # count\n",
    "    res = wordsRDD.map(lambda w: (w, 1)).reduceByKey(lambda x,y: x+y).collect()\n",
    "    \n",
    "    # send result (unique words + count) back to another Kafka topic\n",
    "    p = Producer(server='kafka', port=9092)\n",
    "    o['counted_words'] = res\n",
    "    p.send('adjectives_counted', 'mykey', json.dumps(o))\n",
    "    \n",
    "from Kafka_Helpers import Consumer, Producer\n",
    "\n",
    "Consumer(server='kafka', port=9092, topic_name='adjectives', handler=count_and_send_back)\n",
    "# Consumer(server='kafka', port=9092, topic_name='topic_d', handler=print_key_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30eab4c-4158-4122-98da-65355633b1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
