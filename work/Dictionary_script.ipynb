{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create dictionary full of adjectives and insert them into the key-value db redis\n",
    "\n",
    "## Parse dictionary files (json and custom file format) & Save adjectives in Redis DB & Listen to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7503003-f942-447a-bc0f-11cdff054e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edged', 'semiotic', 'undereducated', 'flustered', 'fortunate', 'subclavian', 'adonic', 'glabrescent', 'bloodstained', 'privileged']\n",
      "Waiting for new events...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import redis\n",
    "import json\n",
    "from Kafka_Helpers import Producer, Consumer\n",
    "\n",
    "# Natural Language Toolkit already offers many libraries regarding computer linguistics\n",
    "# E.g. \"from nltk.corpus import wordnet as wn\" could be used to categorise the part of speech of a given word\n",
    "# For education's sake, we try to create our own list of adjectives using Wordnet and/or a json file\n",
    "\n",
    "# allowed_specials (syntactic markers)\n",
    "# p predicate position\n",
    "# a prenominal (attributive) position\n",
    "# ip immediately postnominal position\n",
    "def read_word_net_dictionary(file_path, allowed_specials = [], allow_adjective_satellite = True):\n",
    "    adjectives_word_net = set()\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        #https://wordnet.princeton.edu/documentation/wndb5wn\n",
    "        #https://wordnet.princeton.edu/documentation/wninput5wn\n",
    "        # word with opt. info (.) followed by space + hexCode + space\n",
    "        match = re.search(r\"\\d+ \\w{2} (\\w) \\w{2} ((?:[a-zA-Z_\\-.']+(?:\\((a|p|ip)\\))? [0-9a-fA-F] )+)\", line)\n",
    "        if match is not None:\n",
    "            # between words is a one-digit hex code distinctly identifying a word within a lexicographer's file\n",
    "            words = re.sub(r\" [0-9a-fA-F] \", \" \", match.group(2)).strip()\n",
    "            # replace multiple spaces with single space\n",
    "            words = re.sub(r\" +\", \" \", words).split(\" \")\n",
    "            for word in words:\n",
    "                # _ in word means space -> two words | remove all words which are adjectives only in a certain context | potentially remove adjective satellite\n",
    "                syntactic_marker_match = re.search(r'\\((.{1,2})\\)$', word)\n",
    "                # if no markers exist or this marker is whitelisted\n",
    "                is_marker_allowed = syntactic_marker_match is None or syntactic_marker_match.group(1) in allowed_specials\n",
    "                # if adjective satellites are allowed, or it isn't an adjective satellite\n",
    "                show_adjective_satellites = allow_adjective_satellite or match.group(1) != 's'\n",
    "                if \"_\" not in word and show_adjective_satellites and is_marker_allowed:\n",
    "                    word = re.sub(r\"\\(.+\\)\", \"\", word)\n",
    "                    adjectives_word_net.add(word.lower())\n",
    "    return adjectives_word_net\n",
    "\n",
    "\n",
    "def read_nltk_extraction(directory_path):\n",
    "    adjectives_nltk = set()\n",
    "\n",
    "    # traverse directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        # if path leads to file\n",
    "        if os.path.isfile(file_path):\n",
    "            # open file and read as json\n",
    "            with open(file_path, \"r\") as file:\n",
    "                file_json = json.load(file)\n",
    "            # loop through json dictionary entries\n",
    "            for key, value in file_json.items():\n",
    "                # if meanings is empty\n",
    "                if value['MEANINGS'] is None or not value['MEANINGS']:\n",
    "                    continue\n",
    "                for key_meaning, value_meaning in value['MEANINGS'].items():\n",
    "                    # only take first meaning into consideration - if not enough adjectives are found, evaluate better approach\n",
    "                    # e.g. by taking words whose adjective-meanings make up >= 50% of all meanings\n",
    "                    if value_meaning[0].lower() == 'adjective':\n",
    "                        adjectives_nltk.add(key.lower())\n",
    "                    break\n",
    "    return adjectives_nltk\n",
    "\n",
    "\n",
    "def flush_and_fill_redis(redis_connection, adjectives_dict):\n",
    "    # remove existing entries\n",
    "    redis_connection.flushdb()\n",
    "    pipe = redis_connection.pipeline() # create a pipeline instance\n",
    "    for adjective in adjectives_dict:\n",
    "        # insert adjective into redis db\n",
    "        # the value is irrelevant if single keys are accessed but has to be the word if redis pipelines are used during retrieval\n",
    "        pipe.set(adjective, adjective)\n",
    "    pipe.execute()  # the EXECUTE call sends all buffered commands to the server\n",
    "\n",
    "    print(f'Number of adjectives: {len(adjectives_dict)}')\n",
    "\n",
    "    # test - value is string if not None (weird, but okay)\n",
    "    assert redis_connection.get('excited') == 'excited'\n",
    "    assert redis_connection.get('house') is None\n",
    "\n",
    "# all adjectives + markers\n",
    "#adjectives = read_word_net_dictionary(\"./dictionary_script/data/adjectives_wordnet.adj\", ['a', 'ip', 'p'], True)\n",
    "\n",
    "# without adjective satellites\n",
    "#adjectives = read_word_net_dictionary(\"./dictionary_script/data/adjectives_wordnet.adj\", [], False)\n",
    "\n",
    "# using dictionary json\n",
    "adjectives  = read_nltk_extraction('./dictionary_script/data/Dictionary JSON')\n",
    "\n",
    "# preview\n",
    "print(list(adjectives)[0:10])\n",
    "\n",
    "# create connection to redis db\n",
    "redis_conn = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)\n",
    "\n",
    "# flush and fill redis if no entries are present\n",
    "# (flushing in case the condition is removed)\n",
    "if redis_conn.dbsize() == 0:\n",
    "    flush_and_fill_redis(redis_conn, adjectives)\n",
    "\n",
    "# fill blacklist if necessary\n",
    "blacklist = { }\n",
    "\n",
    "dictionary_producer = Producer('localhost', 29092)\n",
    "\n",
    "\n",
    "def subscribe_handler(key, value):\n",
    "    infos_and_reviews = json.loads(value)\n",
    "    reviews = infos_and_reviews['reviews']\n",
    "    print('got')\n",
    "    pipe = redis_conn.pipeline()  # create a pipeline instance\n",
    "\n",
    "    # ignore result which is going to be a list of some sort of pipeline objects\n",
    "    [pipe.get(word) for review in reviews for word in review]  # for each word call get on pipe\n",
    "\n",
    "    adjectives_in_reviews = set(pipe.execute())  # send pipe buffer at once and receive all adjectives in reviews\n",
    "    adjectives_in_reviews.remove(None)\n",
    "\n",
    "    # unfortunately, loop through reviews again otherwise the knowledge of which word belongs to which review is lost\n",
    "    # If that is unimportant, adjectives_in_reviews should be a list (not a set)\n",
    "    reviews_words = [[word for word in review if word not in blacklist and word in adjectives_in_reviews] for review in reviews]\n",
    "\n",
    "    #print(adjectives_in_reviews)\n",
    "\n",
    "    dictionary_producer.send('adjectives', key, {\n",
    "        'movie_id': infos_and_reviews['movie_id'],\n",
    "        'title': infos_and_reviews['title'],\n",
    "        'reviews': reviews_words\n",
    "    })\n",
    "\n",
    "\n",
    "dictionary_consumer = Consumer('localhost', 29092, 'movie_reviews', subscribe_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import requests\n",
    "from secrets import api_key\n",
    "import json\n",
    "from Kafka_Helpers import Producer, Consumer\n",
    "import re\n",
    "\n",
    "#api_key = \"105864a59e519ef281a74ca3af6c1b17\"\n",
    "\n",
    "test_producer = Producer('localhost', 29092)\n",
    "\n",
    "#request = requests.get(\"https://api.themoviedb.org/3/movie/top_rated?language=en-US\", {'api_key': api_key})\n",
    "#response = request.json()\n",
    "#first_result = response['results'][0]\n",
    "movie_id = 76600 # first_result['id']\n",
    "title = 'Avatar: The Way of Water' # first_result['title']\n",
    "\n",
    "request = requests.get(f\"https://api.themoviedb.org/3/movie/{movie_id}/reviews?language=en-US\", {'api_key': api_key})\n",
    "response = request.json()\n",
    "\n",
    "reviews = [review['content'] for review in response['results']]\n",
    "result = [re.sub(r\"[^\\w \\-]\", \"\", review.lower()).split(\" \") for review in reviews]\n",
    "\n",
    "test_producer.send(\"movie_reviews\", \"key\", json.dumps({\n",
    "    \"movie_id\": movie_id,\n",
    "    \"title\": title,\n",
    "    \"reviews\": result\n",
    "}))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
